{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 366,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StEocuj3Zzqx",
        "outputId": "78bd5585-d490-4906-bbfa-9555f8360cc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[toy_text] in /usr/local/lib/python3.9/dist-packages (0.25.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym[toy_text]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym[toy_text]) (6.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym[toy_text]) (2.2.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym[toy_text]) (1.22.4)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.9/dist-packages (from gym[toy_text]) (2.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym[toy_text]) (3.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.9/dist-packages (2.1.0)\n"
          ]
        }
      ],
      "source": [
        "#Installing missing libraries\n",
        "!pip install gym[toy_text]\n",
        "!pip install pygame "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueID8I6z9739"
      },
      "source": [
        "# 1.Import the required libraries: numpy and gym\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 367,
      "metadata": {
        "id": "HPWAul6m4nPX"
      },
      "outputs": [],
      "source": [
        "#  numerical computing library in Python that provides tools for working with arrays and matrices\n",
        "import numpy as np\n",
        "# library for developing video games and multimedia applications in Python. It provides a wide range of features for handling graphics, audio, and user input, making it a powerful tool for creating interactive programs.\n",
        "import pygame\n",
        "# submodule of the pygame library that defines various constants and key codes used for handling user input events in Pygame.\n",
        "from pygame.locals import *\n",
        "#  provides a way to interact with the underlying operating system in a platform-independent way. It provides functions for working with files and directories, running external commands, and accessing environment variables.\n",
        "import os\n",
        "# Stop future warning\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "# library for developing and comparing reinforcement learning algorithms\n",
        "import gym\n",
        "import pandas as pd\n",
        "#  sets the value of the SDL_VIDEODRIVER environment variable to 'dummy'\n",
        "os.environ['SDL_VIDEODRIVER']='dummy'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNBNZ3XK-C8v"
      },
      "source": [
        "# 2. Initialize the environment and reset the current one. Set is_slippery=False in the initializer. Show the size of the action space and the number of possible states.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 368,
      "metadata": {
        "id": "YTI6P1oB4xB8"
      },
      "outputs": [],
      "source": [
        "def environment_initialization():\n",
        "  #  creates an instance of the FrozenLake-v1 environment in OpenAI Gym.The FrozenLake-v1 environment is a gridworld game where the agent must navigate from a starting position to a goal position  while avoiding holes  that cause the agent to fall in and fail. The map for this environment is a 4x4 grid.\n",
        "    environment = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False, render_mode=\"human\")\n",
        "    # reset the environment to its initial state and return the starting state to the agent.\n",
        "    environment.reset()\n",
        "    return environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 369,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeU4Hyd6gDNM",
        "outputId": "d5cb62f9-11c3-4dab-d67e-cec6e2e3d07e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Space of StateDiscrete(16)\n",
            "Space of ActionDiscrete(4)\n"
          ]
        }
      ],
      "source": [
        "environment= environment_initialization()\n",
        " # There will be 16 states 4*4 grid\n",
        "print(\"Space of State\" +str(environment.observation_space))\n",
        "# in each cell there will be 4 possible actions in a grid-- left,right,up,down\n",
        "print(\"Space of Action\"+str(environment.action_space))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fyjb45yC8j6_"
      },
      "source": [
        "# 3. Perform policy evaluation iterations until the smallest change is less  than smallest_change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 370,
      "metadata": {
        "id": "cXEbEsJQ41sH"
      },
      "outputs": [],
      "source": [
        "# evaluate the current policy and update the value function until it converges. The algorithm used in this function is called policy evaluation, which involves repeatedly updating the value function until it stops changing. The convergence is determined by checking whether the maximum change in the value function over all states is less than the threshold s_change.\n",
        "def evaluate_policy(value, policy_right_now,\n",
        "                      environment, discount_factor_gamma, s_change):\n",
        "  # while the change is negligible and acceptable this loop will iterate and will get terminate once the change is negligible\n",
        "    while True:\n",
        "        b_change = 0\n",
        "        # traverse through all possible state present, id for observation space\n",
        "        for item in range(environment.observation_space.n):\n",
        "            # decide the action according to chosen policy\n",
        "            a = policy_right_now[item]\n",
        "            # assign the current value of current state to a temporary variable\n",
        "            ov = value[item]\n",
        "            # extracts the transition probabilities for a given state item and action a. The next_state variable represents the next state the agent will be in after taking the action a, r represents the reward received by the agent for taking action a in state item, and probability represents the probability of transitioning to the next_state after taking action a. is_done is a boolean value indicating whether the episode has ended or not.\n",
        "            probability, next_state, r, is_done = environment.env.P[item][a][0]\n",
        "            # bellman optimality equation\n",
        "            #  value is a list or array that stores the estimated values of each state in the environment. The value of item is updated using the Bellman equation, which states that the value of a state is equal to the immediate reward received in that state (r) plus the discounted value of the next state (discount_factor_gamma * value[next_state]), where discount_factor_gamma is a discount factor that trades off immediate rewards and future rewards.\n",
        "            value[item] = r + discount_factor_gamma * value[next_state]\n",
        "            # computes the maximum absolute change in the value function value during a policy evaluation step of a reinforcement learning algorithm.\n",
        "            b_change = max(b_change, abs(value[item] - ov))\n",
        "         # stop the iteration if the change is ignorable\n",
        "        if b_change < s_change:\n",
        "            break\n",
        "            # returns the value\n",
        "    return value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TNewdZc-Ncv"
      },
      "source": [
        "# 4. Perform policy improvement using the Bellman optimality equation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 371,
      "metadata": {
        "id": "tzbfMu0Z472l"
      },
      "outputs": [],
      "source": [
        "#  improve the current policy based on the given value function. The algorithm used in this function is called policy iteration, which involves iteratively improving the policy and evaluating the new policy until it converges to an optimal policy.\n",
        "def improve_policy(v, policy_right_now,environment, discount_factor_gamma):\n",
        "  # Assign new policy as false to reset policy\n",
        "    is_policy_changed = False\n",
        "    # iterates over all possible states in the FrozenLake-v1 environment.\n",
        "    for s in range(environment.observation_space.n):\n",
        "      # assign best value as infinity as an initial value\n",
        "        b_val = -np.inf\n",
        "        # assign best action as -1 as an initial value, which is an invalid action\n",
        "        b_action = -1\n",
        "        #  iterates through over all possible actions in the FrozenLake-v1 environment.Left,Right,Up,Down\n",
        "        for a in range(environment.action_space.n):\n",
        "            probability, next_state, r, is_done = environment.env.P[s][a][0]\n",
        "            # Consider current action and calculatr future reward\n",
        "            fr = r + discount_factor_gamma * v[next_state]\n",
        "            if fr > b_val:\n",
        "                b_action = a\n",
        "                b_val = fr\n",
        "        assert b_action != -1\n",
        "        # upon the policy changes is_policy_changed variable get flaged \n",
        "        if policy_right_now[s] != b_action:\n",
        "            is_policy_changed = True\n",
        "        # identify and update suitable action for the current state\n",
        "        policy_right_now[s] = b_action\n",
        "        # returns current policy and a flag to notify that the policy changed\n",
        "    return policy_right_now, is_policy_changed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5WgsiPy-SVV"
      },
      "source": [
        "# 5. Find the most optimal policy for the FrozenLake-v1 environment using policy iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 372,
      "metadata": {
        "id": "IPuaqha25CYb"
      },
      "outputs": [],
      "source": [
        "#  takes as input the FrozenLake-v1 environment environment. The function initializes the value function value to all zeros and the policy policy_right_now to a random policy. The function then performs policy iteration until the policy converges to an optimal policy.\n",
        "def iterate_policy(environment):\n",
        "    epis = 0\n",
        "    #discount factorshould be between 0-1, allocate future coefficient\n",
        "    discount_factor_gamma = 0.8\n",
        "    # initialize possible smallest change \n",
        "    s_change = 1e-20\n",
        "    # allocate value as empty dictornary\n",
        "    value = dict()\n",
        "    # allocate current policy as empty dictornary\n",
        "    policy_right_now = dict()\n",
        "    #  iterates over all possible states in the FrozenLake-v1 environment using the range function and the observation_space.n attribute of the environment.\n",
        "    for state in range(environment.observation_space.n):\n",
        "      # apply random policy \n",
        "        value[state] = np.random.random()\n",
        "        policy_right_now[state] = environment.action_space.sample()\n",
        "    #iterate through episodes, the allocated episodes are 70000\n",
        "    while epis < 70000:\n",
        "        # updating the value function value based on the given policy policy_right_now and the observed transitions in the environment.\n",
        "        value = evaluate_policy(value, policy_right_now,\n",
        "                              environment, discount_factor_gamma, s_change)\n",
        "        # improve policy targetting to maximum reward\n",
        "        policy_right_now, is_policy_changed = improve_policy(value, policy_right_now,\n",
        "                                                            environment, discount_factor_gamma)\n",
        "        # incrementing to next episode\n",
        "        epis += 1\n",
        "        # If there are no policy changes breaking the loop\n",
        "        if not is_policy_changed:\n",
        "            break\n",
        "    print (\"Trained Number of Episodes: \"+str(epis))\n",
        "    return policy_right_now"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PnCdA6K-WJc"
      },
      "source": [
        "# 6. Perform a test pass on the FrozenLake-v1 environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 373,
      "metadata": {
        "id": "4D48y6hm5IsL"
      },
      "outputs": [],
      "source": [
        "def test_pass(plcy, rndr=False):\n",
        "  # initializing environment\n",
        "    environment = environment_initialization()\n",
        "    rds = []\n",
        "    test_epis = 300\n",
        "    for episode in range(test_epis):\n",
        "        # reset the environmentfor each and every new episode\n",
        "        s = environment.reset()\n",
        "        # assign total rewards 0 at the initial stage\n",
        "        ttl_rwds = 0\n",
        "        # print(\"#\" * 120)\n",
        "        print(\"Episode No\"+str(episode))\n",
        "\n",
        "        # The number of maximum allowed steps to perform\n",
        "        for stp in range(65):\n",
        "            # Choose action with highest q value in the  state right now\n",
        "            next_state, rwd, dn, infomation = environment.step(plcy[s])\n",
        "            if rndr:\n",
        "                environment.render()\n",
        "                # sum up total rewards\n",
        "            ttl_rwds += rwd\n",
        "            if dn:\n",
        "                rds.append(ttl_rwds)\n",
        "                print(\"Score:-\", ttl_rwds)\n",
        "                break\n",
        "            s = next_state\n",
        "    average_score = sum(rds) / test_epis\n",
        "    print(\"Avg Score:\", average_score)\n",
        "    environment.close()\n",
        "    # the average score will be returned\n",
        "    return average_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vt5NHLe-cY7"
      },
      "source": [
        "# 7. Take steps through the FrozenLake-v1 environment randomly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 374,
      "metadata": {
        "id": "LcUr1x5m5Mdf"
      },
      "outputs": [],
      "source": [
        "def take_steps_randomly(number_of_steps=5):\n",
        "    # Initialize environment and reset\n",
        "    environment = environment_initialization()\n",
        "    st = environment.reset()\n",
        "    for step in range(number_of_steps):\n",
        "        # Randomly choose action\n",
        "        actn = environment.action_space.sample()\n",
        "        environment.render()\n",
        "        #Get Reward ,Next step, is_done and information\n",
        "        next_state, rwd, dn, information = environment.step(actn)\n",
        "        print(\"Next State: \"+str(next_state)+\"\\n\"+\"Reward: \"+str(rwd)+\"\\n\"+\"is_done: \"+str(dn)+\"\\n\"+\"Infomation: \"+str(information)+\"\\n\")\n",
        "        # print(\"#\" * 40)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iO44w3ip-g7q"
      },
      "source": [
        "# 8. Perform value iteration to find the most optimal policy for the FrozenLake-v1 environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 375,
      "metadata": {
        "id": "nHfxztZ_ZH84"
      },
      "outputs": [],
      "source": [
        "def iterate_value(environment):\n",
        "  # create an empty dictornary for value\n",
        "    value= dict()\n",
        "     # create an empty dictornary for policy\n",
        "    plcy = dict()\n",
        "    size_of_action = environment.action_space.n\n",
        "    # discount factor it can be 0-1 according to the reward we expect either immediate or futuristic\n",
        "    discount_factor_gamma = 0.8\n",
        "    #Iterate through observation space\n",
        "    for item in range(environment.observation_space.n):\n",
        "      #  setting a random action since it follows random policy\n",
        "        plcy[item] = environment.action_space.sample()\n",
        "        # assign value as -1 for all observation state at initial\n",
        "        value[item] = -1\n",
        "    #Iterate until the change in value function is negligible\n",
        "    while True:\n",
        "      # assign initial delta value as 0\n",
        "        dlta = 0\n",
        "        # iterate over a range of values in reverse order. The range function generates a sequence of integers, and reversed returns an iterator that goes over the sequence in reverse order.\n",
        "        for itr in reversed(range(environment.observation_space.n)):\n",
        "          # assign old value to new temporary variable for later usage\n",
        "            o_v = value[itr]\n",
        "            # reset the best action\n",
        "            b_action = None\n",
        "            # reset best rewards\n",
        "            b_rewards = -np.inf\n",
        "            # Iterate through all the actions in current state\n",
        "            for actn in range(size_of_action):\n",
        "               # Get probability , next state,reward, and is_done\n",
        "                probability, next_state, rwd, dn = environment.env.P[itr][actn][0]\n",
        "                # possible reward calculated using imediate reward + callculated future reward after discounted\n",
        "                p_reward = rwd + discount_factor_gamma * value[next_state]\n",
        "                # choose the best reward  and  choose the action to the policy accordingly\n",
        "                if p_reward > b_rewards:\n",
        "                    b_rewards = p_reward\n",
        "                    b_action = actn\n",
        "            # assign best reward  for that perticular state\n",
        "            value[itr] = b_rewards\n",
        "            # assign best action  for that perticular state\n",
        "            plcy[itr] = b_action\n",
        "            dlta = max(dlta, abs(value[itr] - o_v))\n",
        "         # terminate the loop if change is ignorable\n",
        "        #  1e-30<- possible smallext value in python\n",
        "        if dlta < 1e-30:\n",
        "            break\n",
        "    print(plcy)\n",
        "    print(value)\n",
        "    return plcy,value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 380,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "lCF4dXiwE-d9",
        "outputId": "5aca0aec-cfc3-4375-c295-5bc927ae5c3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trained Number of Episodes: 6\n",
            "Policy\n",
            "{0: 1, 1: 2, 2: 1, 3: 0, 4: 1, 5: 0, 6: 1, 7: 0, 8: 2, 9: 1, 10: 1, 11: 0, 12: 0, 13: 2, 14: 2, 15: 0}\n",
            "[1 2 1 0 1 0 1 0 2 1 1 0 0 2 2 0]\n",
            "[[1 2 1 0]\n",
            " [1 0 1 0]\n",
            " [2 1 1 0]\n",
            " [0 2 2 0]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "execution_count": 380,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 288x216 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "environnment = environment_initialization()\n",
        "policy = iterate_policy(environnment)\n",
        "print(\"Policy\")\n",
        "print(policy)\n",
        "arr=[]\n",
        "# convert policy into array\n",
        "for item in policy:\n",
        "  arr.append(policy[item])\n",
        "  # convert array into numpy array\n",
        "arr=np.array(arr)\n",
        "print(arr)\n",
        "# reshape (1,16) array to(4,4) array\n",
        "arr = arr.reshape(4,4)\n",
        "print(arr)\n",
        "# assign array into dataframe\n",
        "df =pd.DataFrame.from_dict(arr) \n",
        "# plot the policy by state\n",
        "fig, ax = plt.subplots(figsize=(4, 3)) \n",
        "sns.heatmap(df, annot=True, cmap='magma')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 381,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        },
        "id": "BGH9S02fFI3W",
        "outputId": "4bb41b24-a702-4dee-d483-84ea297efcba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 1, 1: 2, 2: 1, 3: 0, 4: 1, 5: 0, 6: 1, 7: 0, 8: 2, 9: 1, 10: 1, 11: 0, 12: 0, 13: 2, 14: 2, 15: 0}\n",
            "{0: 0.32768000000000014, 1: 0.40960000000000013, 2: 0.5120000000000001, 3: 0.40960000000000013, 4: 0.40960000000000013, 5: -3.4622310392507545e-30, 6: 0.6400000000000001, 7: -3.4622310392507545e-30, 8: 0.5120000000000001, 9: 0.6400000000000001, 10: 0.8, 11: -3.4622310392507545e-30, 12: -3.4622310392507545e-30, 13: 0.8, 14: 1.0, 15: -3.4622310392507545e-30}\n",
            "Policy\n",
            "{0: 1, 1: 2, 2: 1, 3: 0, 4: 1, 5: 0, 6: 1, 7: 0, 8: 2, 9: 1, 10: 1, 11: 0, 12: 0, 13: 2, 14: 2, 15: 0}\n",
            "value\n",
            "{0: 0.32768000000000014, 1: 0.40960000000000013, 2: 0.5120000000000001, 3: 0.40960000000000013, 4: 0.40960000000000013, 5: -3.4622310392507545e-30, 6: 0.6400000000000001, 7: -3.4622310392507545e-30, 8: 0.5120000000000001, 9: 0.6400000000000001, 10: 0.8, 11: -3.4622310392507545e-30, 12: -3.4622310392507545e-30, 13: 0.8, 14: 1.0, 15: -3.4622310392507545e-30}\n",
            "[1 2 1 0 1 0 1 0 2 1 1 0 0 2 2 0]\n",
            "[[1 2 1 0]\n",
            " [1 0 1 0]\n",
            " [2 1 1 0]\n",
            " [0 2 2 0]]\n",
            "[1 2 1 0 1 0 1 0 2 1 1 0 0 2 2 0]\n",
            "[[1 2 1 0]\n",
            " [1 0 1 0]\n",
            " [2 1 1 0]\n",
            " [0 2 2 0]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "execution_count": 381,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 288x216 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 288x216 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "policy,value = iterate_value(environnment)\n",
        "print(\"Policy\")\n",
        "print(policy)\n",
        "print(\"value\")\n",
        "print(value)\n",
        "arr=[]\n",
        "# convert policy into array\n",
        "for item in policy:\n",
        "  arr.append(policy[item])\n",
        "  # convert array into numpy array\n",
        "arr=np.array(arr)\n",
        "print(arr)\n",
        "# reshape (1,16) array to(4,4) array\n",
        "arr = arr.reshape(4,4)\n",
        "print(arr)\n",
        "# assign array into dataframe\n",
        "df =pd.DataFrame.from_dict(arr) \n",
        "# plot the policy by state\n",
        "fig, ax = plt.subplots(figsize=(4, 3)) \n",
        "sns.heatmap(df, annot=True, cmap='magma')  \n",
        "\n",
        "\n",
        "arr=[]\n",
        "# convert value into array\n",
        "for item in value:\n",
        "  arr.append(policy[item])\n",
        "  # convert array into numpy array\n",
        "arr=np.array(arr)\n",
        "print(arr)\n",
        "# reshape (1,16) array to(4,4) array\n",
        "arr = arr.reshape(4,4)\n",
        "print(arr)\n",
        "# assign array into dataframe\n",
        "df =pd.DataFrame.from_dict(arr) \n",
        "# plot the policy by state\n",
        "fig, ax = plt.subplots(figsize=(4, 3)) \n",
        "sns.heatmap(df, annot=True, cmap='magma')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 378,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1Qs0JJK4Ypm",
        "outputId": "cb0d0976-997d-43f1-8c19-d528dfdd4414"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode No0\n",
            "Score:- 1.0\n",
            "Episode No1\n",
            "Score:- 1.0\n",
            "Episode No2\n",
            "Score:- 1.0\n",
            "Episode No3\n",
            "Score:- 1.0\n",
            "Episode No4\n",
            "Score:- 1.0\n",
            "Episode No5\n",
            "Score:- 1.0\n",
            "Episode No6\n",
            "Score:- 1.0\n",
            "Episode No7\n",
            "Score:- 1.0\n",
            "Episode No8\n",
            "Score:- 1.0\n",
            "Episode No9\n",
            "Score:- 1.0\n",
            "Episode No10\n",
            "Score:- 1.0\n",
            "Episode No11\n",
            "Score:- 1.0\n",
            "Episode No12\n",
            "Score:- 1.0\n",
            "Episode No13\n",
            "Score:- 1.0\n",
            "Episode No14\n",
            "Score:- 1.0\n",
            "Episode No15\n",
            "Score:- 1.0\n",
            "Episode No16\n",
            "Score:- 1.0\n",
            "Episode No17\n",
            "Score:- 1.0\n",
            "Episode No18\n",
            "Score:- 1.0\n",
            "Episode No19\n",
            "Score:- 1.0\n",
            "Episode No20\n",
            "Score:- 1.0\n",
            "Episode No21\n",
            "Score:- 1.0\n",
            "Episode No22\n",
            "Score:- 1.0\n",
            "Episode No23\n",
            "Score:- 1.0\n",
            "Episode No24\n",
            "Score:- 1.0\n",
            "Episode No25\n",
            "Score:- 1.0\n",
            "Episode No26\n",
            "Score:- 1.0\n",
            "Episode No27\n",
            "Score:- 1.0\n",
            "Episode No28\n",
            "Score:- 1.0\n",
            "Episode No29\n",
            "Score:- 1.0\n",
            "Episode No30\n",
            "Score:- 1.0\n",
            "Episode No31\n",
            "Score:- 1.0\n",
            "Episode No32\n",
            "Score:- 1.0\n",
            "Episode No33\n",
            "Score:- 1.0\n",
            "Episode No34\n",
            "Score:- 1.0\n",
            "Episode No35\n",
            "Score:- 1.0\n",
            "Episode No36\n",
            "Score:- 1.0\n",
            "Episode No37\n",
            "Score:- 1.0\n",
            "Episode No38\n",
            "Score:- 1.0\n",
            "Episode No39\n",
            "Score:- 1.0\n",
            "Episode No40\n",
            "Score:- 1.0\n",
            "Episode No41\n",
            "Score:- 1.0\n",
            "Episode No42\n",
            "Score:- 1.0\n",
            "Episode No43\n",
            "Score:- 1.0\n",
            "Episode No44\n",
            "Score:- 1.0\n",
            "Episode No45\n",
            "Score:- 1.0\n",
            "Episode No46\n",
            "Score:- 1.0\n",
            "Episode No47\n",
            "Score:- 1.0\n",
            "Episode No48\n",
            "Score:- 1.0\n",
            "Episode No49\n",
            "Score:- 1.0\n",
            "Episode No50\n",
            "Score:- 1.0\n",
            "Episode No51\n",
            "Score:- 1.0\n",
            "Episode No52\n",
            "Score:- 1.0\n",
            "Episode No53\n",
            "Score:- 1.0\n",
            "Episode No54\n",
            "Score:- 1.0\n",
            "Episode No55\n",
            "Score:- 1.0\n",
            "Episode No56\n",
            "Score:- 1.0\n",
            "Episode No57\n",
            "Score:- 1.0\n",
            "Episode No58\n",
            "Score:- 1.0\n",
            "Episode No59\n",
            "Score:- 1.0\n",
            "Episode No60\n",
            "Score:- 1.0\n",
            "Episode No61\n",
            "Score:- 1.0\n",
            "Episode No62\n",
            "Score:- 1.0\n",
            "Episode No63\n",
            "Score:- 1.0\n",
            "Episode No64\n",
            "Score:- 1.0\n",
            "Episode No65\n",
            "Score:- 1.0\n",
            "Episode No66\n",
            "Score:- 1.0\n",
            "Episode No67\n",
            "Score:- 1.0\n",
            "Episode No68\n",
            "Score:- 1.0\n",
            "Episode No69\n",
            "Score:- 1.0\n",
            "Episode No70\n",
            "Score:- 1.0\n",
            "Episode No71\n",
            "Score:- 1.0\n",
            "Episode No72\n",
            "Score:- 1.0\n",
            "Episode No73\n",
            "Score:- 1.0\n",
            "Episode No74\n",
            "Score:- 1.0\n",
            "Episode No75\n",
            "Score:- 1.0\n",
            "Episode No76\n",
            "Score:- 1.0\n",
            "Episode No77\n",
            "Score:- 1.0\n",
            "Episode No78\n",
            "Score:- 1.0\n",
            "Episode No79\n",
            "Score:- 1.0\n",
            "Episode No80\n",
            "Score:- 1.0\n",
            "Episode No81\n",
            "Score:- 1.0\n",
            "Episode No82\n",
            "Score:- 1.0\n",
            "Episode No83\n",
            "Score:- 1.0\n",
            "Episode No84\n",
            "Score:- 1.0\n",
            "Episode No85\n",
            "Score:- 1.0\n",
            "Episode No86\n",
            "Score:- 1.0\n",
            "Episode No87\n",
            "Score:- 1.0\n",
            "Episode No88\n",
            "Score:- 1.0\n",
            "Episode No89\n",
            "Score:- 1.0\n",
            "Episode No90\n",
            "Score:- 1.0\n",
            "Episode No91\n",
            "Score:- 1.0\n",
            "Episode No92\n",
            "Score:- 1.0\n",
            "Episode No93\n",
            "Score:- 1.0\n",
            "Episode No94\n",
            "Score:- 1.0\n",
            "Episode No95\n",
            "Score:- 1.0\n",
            "Episode No96\n",
            "Score:- 1.0\n",
            "Episode No97\n",
            "Score:- 1.0\n",
            "Episode No98\n",
            "Score:- 1.0\n",
            "Episode No99\n",
            "Score:- 1.0\n",
            "Episode No100\n",
            "Score:- 1.0\n",
            "Episode No101\n",
            "Score:- 1.0\n",
            "Episode No102\n",
            "Score:- 1.0\n",
            "Episode No103\n",
            "Score:- 1.0\n",
            "Episode No104\n",
            "Score:- 1.0\n",
            "Episode No105\n",
            "Score:- 1.0\n",
            "Episode No106\n",
            "Score:- 1.0\n",
            "Episode No107\n",
            "Score:- 1.0\n",
            "Episode No108\n",
            "Score:- 1.0\n",
            "Episode No109\n",
            "Score:- 1.0\n",
            "Episode No110\n",
            "Score:- 1.0\n",
            "Episode No111\n",
            "Score:- 1.0\n",
            "Episode No112\n",
            "Score:- 1.0\n",
            "Episode No113\n",
            "Score:- 1.0\n",
            "Episode No114\n",
            "Score:- 1.0\n",
            "Episode No115\n",
            "Score:- 1.0\n",
            "Episode No116\n",
            "Score:- 1.0\n",
            "Episode No117\n",
            "Score:- 1.0\n",
            "Episode No118\n",
            "Score:- 1.0\n",
            "Episode No119\n",
            "Score:- 1.0\n",
            "Episode No120\n",
            "Score:- 1.0\n",
            "Episode No121\n",
            "Score:- 1.0\n",
            "Episode No122\n",
            "Score:- 1.0\n",
            "Episode No123\n",
            "Score:- 1.0\n",
            "Episode No124\n",
            "Score:- 1.0\n",
            "Episode No125\n",
            "Score:- 1.0\n",
            "Episode No126\n",
            "Score:- 1.0\n",
            "Episode No127\n",
            "Score:- 1.0\n",
            "Episode No128\n",
            "Score:- 1.0\n",
            "Episode No129\n",
            "Score:- 1.0\n",
            "Episode No130\n",
            "Score:- 1.0\n",
            "Episode No131\n",
            "Score:- 1.0\n",
            "Episode No132\n",
            "Score:- 1.0\n",
            "Episode No133\n",
            "Score:- 1.0\n",
            "Episode No134\n",
            "Score:- 1.0\n",
            "Episode No135\n",
            "Score:- 1.0\n",
            "Episode No136\n",
            "Score:- 1.0\n",
            "Episode No137\n",
            "Score:- 1.0\n",
            "Episode No138\n",
            "Score:- 1.0\n",
            "Episode No139\n",
            "Score:- 1.0\n",
            "Episode No140\n",
            "Score:- 1.0\n",
            "Episode No141\n",
            "Score:- 1.0\n",
            "Episode No142\n",
            "Score:- 1.0\n",
            "Episode No143\n",
            "Score:- 1.0\n",
            "Episode No144\n",
            "Score:- 1.0\n",
            "Episode No145\n",
            "Score:- 1.0\n",
            "Episode No146\n",
            "Score:- 1.0\n",
            "Episode No147\n",
            "Score:- 1.0\n",
            "Episode No148\n",
            "Score:- 1.0\n",
            "Episode No149\n",
            "Score:- 1.0\n",
            "Episode No150\n",
            "Score:- 1.0\n",
            "Episode No151\n",
            "Score:- 1.0\n",
            "Episode No152\n",
            "Score:- 1.0\n",
            "Episode No153\n",
            "Score:- 1.0\n",
            "Episode No154\n",
            "Score:- 1.0\n",
            "Episode No155\n",
            "Score:- 1.0\n",
            "Episode No156\n",
            "Score:- 1.0\n",
            "Episode No157\n",
            "Score:- 1.0\n",
            "Episode No158\n",
            "Score:- 1.0\n",
            "Episode No159\n",
            "Score:- 1.0\n",
            "Episode No160\n",
            "Score:- 1.0\n",
            "Episode No161\n",
            "Score:- 1.0\n",
            "Episode No162\n",
            "Score:- 1.0\n",
            "Episode No163\n",
            "Score:- 1.0\n",
            "Episode No164\n",
            "Score:- 1.0\n",
            "Episode No165\n",
            "Score:- 1.0\n",
            "Episode No166\n",
            "Score:- 1.0\n",
            "Episode No167\n",
            "Score:- 1.0\n",
            "Episode No168\n",
            "Score:- 1.0\n",
            "Episode No169\n",
            "Score:- 1.0\n",
            "Episode No170\n",
            "Score:- 1.0\n",
            "Episode No171\n",
            "Score:- 1.0\n",
            "Episode No172\n",
            "Score:- 1.0\n",
            "Episode No173\n",
            "Score:- 1.0\n",
            "Episode No174\n",
            "Score:- 1.0\n",
            "Episode No175\n",
            "Score:- 1.0\n",
            "Episode No176\n",
            "Score:- 1.0\n",
            "Episode No177\n",
            "Score:- 1.0\n",
            "Episode No178\n",
            "Score:- 1.0\n",
            "Episode No179\n",
            "Score:- 1.0\n",
            "Episode No180\n",
            "Score:- 1.0\n",
            "Episode No181\n",
            "Score:- 1.0\n",
            "Episode No182\n",
            "Score:- 1.0\n",
            "Episode No183\n",
            "Score:- 1.0\n",
            "Episode No184\n",
            "Score:- 1.0\n",
            "Episode No185\n",
            "Score:- 1.0\n",
            "Episode No186\n",
            "Score:- 1.0\n",
            "Episode No187\n",
            "Score:- 1.0\n",
            "Episode No188\n",
            "Score:- 1.0\n",
            "Episode No189\n",
            "Score:- 1.0\n",
            "Episode No190\n",
            "Score:- 1.0\n",
            "Episode No191\n",
            "Score:- 1.0\n",
            "Episode No192\n",
            "Score:- 1.0\n",
            "Episode No193\n",
            "Score:- 1.0\n",
            "Episode No194\n",
            "Score:- 1.0\n",
            "Episode No195\n",
            "Score:- 1.0\n",
            "Episode No196\n",
            "Score:- 1.0\n",
            "Episode No197\n",
            "Score:- 1.0\n",
            "Episode No198\n",
            "Score:- 1.0\n",
            "Episode No199\n",
            "Score:- 1.0\n",
            "Episode No200\n",
            "Score:- 1.0\n",
            "Episode No201\n",
            "Score:- 1.0\n",
            "Episode No202\n",
            "Score:- 1.0\n",
            "Episode No203\n",
            "Score:- 1.0\n",
            "Episode No204\n",
            "Score:- 1.0\n",
            "Episode No205\n",
            "Score:- 1.0\n",
            "Episode No206\n",
            "Score:- 1.0\n",
            "Episode No207\n",
            "Score:- 1.0\n",
            "Episode No208\n",
            "Score:- 1.0\n",
            "Episode No209\n",
            "Score:- 1.0\n",
            "Episode No210\n",
            "Score:- 1.0\n",
            "Episode No211\n",
            "Score:- 1.0\n",
            "Episode No212\n",
            "Score:- 1.0\n",
            "Episode No213\n",
            "Score:- 1.0\n",
            "Episode No214\n",
            "Score:- 1.0\n",
            "Episode No215\n",
            "Score:- 1.0\n",
            "Episode No216\n",
            "Score:- 1.0\n",
            "Episode No217\n",
            "Score:- 1.0\n",
            "Episode No218\n",
            "Score:- 1.0\n",
            "Episode No219\n",
            "Score:- 1.0\n",
            "Episode No220\n",
            "Score:- 1.0\n",
            "Episode No221\n",
            "Score:- 1.0\n",
            "Episode No222\n",
            "Score:- 1.0\n",
            "Episode No223\n",
            "Score:- 1.0\n",
            "Episode No224\n",
            "Score:- 1.0\n",
            "Episode No225\n",
            "Score:- 1.0\n",
            "Episode No226\n",
            "Score:- 1.0\n",
            "Episode No227\n",
            "Score:- 1.0\n",
            "Episode No228\n",
            "Score:- 1.0\n",
            "Episode No229\n",
            "Score:- 1.0\n",
            "Episode No230\n",
            "Score:- 1.0\n",
            "Episode No231\n",
            "Score:- 1.0\n",
            "Episode No232\n",
            "Score:- 1.0\n",
            "Episode No233\n",
            "Score:- 1.0\n",
            "Episode No234\n",
            "Score:- 1.0\n",
            "Episode No235\n",
            "Score:- 1.0\n",
            "Episode No236\n",
            "Score:- 1.0\n",
            "Episode No237\n",
            "Score:- 1.0\n",
            "Episode No238\n",
            "Score:- 1.0\n",
            "Episode No239\n",
            "Score:- 1.0\n",
            "Episode No240\n",
            "Score:- 1.0\n",
            "Episode No241\n",
            "Score:- 1.0\n",
            "Episode No242\n",
            "Score:- 1.0\n",
            "Episode No243\n",
            "Score:- 1.0\n",
            "Episode No244\n",
            "Score:- 1.0\n",
            "Episode No245\n",
            "Score:- 1.0\n",
            "Episode No246\n",
            "Score:- 1.0\n",
            "Episode No247\n",
            "Score:- 1.0\n",
            "Episode No248\n",
            "Score:- 1.0\n",
            "Episode No249\n",
            "Score:- 1.0\n",
            "Episode No250\n",
            "Score:- 1.0\n",
            "Episode No251\n",
            "Score:- 1.0\n",
            "Episode No252\n",
            "Score:- 1.0\n",
            "Episode No253\n",
            "Score:- 1.0\n",
            "Episode No254\n",
            "Score:- 1.0\n",
            "Episode No255\n",
            "Score:- 1.0\n",
            "Episode No256\n",
            "Score:- 1.0\n",
            "Episode No257\n",
            "Score:- 1.0\n",
            "Episode No258\n",
            "Score:- 1.0\n",
            "Episode No259\n",
            "Score:- 1.0\n",
            "Episode No260\n",
            "Score:- 1.0\n",
            "Episode No261\n",
            "Score:- 1.0\n",
            "Episode No262\n",
            "Score:- 1.0\n",
            "Episode No263\n",
            "Score:- 1.0\n",
            "Episode No264\n",
            "Score:- 1.0\n",
            "Episode No265\n",
            "Score:- 1.0\n",
            "Episode No266\n",
            "Score:- 1.0\n",
            "Episode No267\n",
            "Score:- 1.0\n",
            "Episode No268\n",
            "Score:- 1.0\n",
            "Episode No269\n",
            "Score:- 1.0\n",
            "Episode No270\n",
            "Score:- 1.0\n",
            "Episode No271\n",
            "Score:- 1.0\n",
            "Episode No272\n",
            "Score:- 1.0\n",
            "Episode No273\n",
            "Score:- 1.0\n",
            "Episode No274\n",
            "Score:- 1.0\n",
            "Episode No275\n",
            "Score:- 1.0\n",
            "Episode No276\n",
            "Score:- 1.0\n",
            "Episode No277\n",
            "Score:- 1.0\n",
            "Episode No278\n",
            "Score:- 1.0\n",
            "Episode No279\n",
            "Score:- 1.0\n",
            "Episode No280\n",
            "Score:- 1.0\n",
            "Episode No281\n",
            "Score:- 1.0\n",
            "Episode No282\n",
            "Score:- 1.0\n",
            "Episode No283\n",
            "Score:- 1.0\n",
            "Episode No284\n",
            "Score:- 1.0\n",
            "Episode No285\n",
            "Score:- 1.0\n",
            "Episode No286\n",
            "Score:- 1.0\n",
            "Episode No287\n",
            "Score:- 1.0\n",
            "Episode No288\n",
            "Score:- 1.0\n",
            "Episode No289\n",
            "Score:- 1.0\n",
            "Episode No290\n",
            "Score:- 1.0\n",
            "Episode No291\n",
            "Score:- 1.0\n",
            "Episode No292\n",
            "Score:- 1.0\n",
            "Episode No293\n",
            "Score:- 1.0\n",
            "Episode No294\n",
            "Score:- 1.0\n",
            "Episode No295\n",
            "Score:- 1.0\n",
            "Episode No296\n",
            "Score:- 1.0\n",
            "Episode No297\n",
            "Score:- 1.0\n",
            "Episode No298\n",
            "Score:- 1.0\n",
            "Episode No299\n",
            "Score:- 1.0\n",
            "Avg Score: 1.0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 378,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_pass(policy, rndr=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 379,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdjd8ko4-kbe",
        "outputId": "bab179ec-b875-4b74-c374-eb82f3a7171c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Next State: 0\n",
            "Reward: 0.0\n",
            "is_done: False\n",
            "Infomation: {'prob': 1.0}\n",
            "\n",
            "Next State: 0\n",
            "Reward: 0.0\n",
            "is_done: False\n",
            "Infomation: {'prob': 1.0}\n",
            "\n",
            "Next State: 4\n",
            "Reward: 0.0\n",
            "is_done: False\n",
            "Infomation: {'prob': 1.0}\n",
            "\n",
            "Next State: 8\n",
            "Reward: 0.0\n",
            "is_done: False\n",
            "Infomation: {'prob': 1.0}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0.0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n",
            "Next State: 12\n",
            "Reward: 0\n",
            "is_done: True\n",
            "Infomation: {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "take_steps_randomly(100)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
